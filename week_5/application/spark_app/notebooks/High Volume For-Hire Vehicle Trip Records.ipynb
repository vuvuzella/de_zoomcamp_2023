{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "962f4041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/25 00:05:23 WARN Utils: Your hostname, vuvuzella-ThinkPad-X1-Extreme resolves to a loopback address: 127.0.1.1; using 192.168.20.18 instead (on interface wlp0s20f3)\n",
      "23/02/25 00:05:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/25 00:05:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"pyspark-notebook\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.executor.memory\", \"2048mb\") \\\n",
    "        .getOrCreate()\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ea036805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-02-23 23:46:54--  https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhvhv/fhvhv_tripdata_2021-01.csv.gz\n",
      "Resolving github.com (github.com)... 20.248.137.48\n",
      "Connecting to github.com (github.com)|20.248.137.48|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/035746e8-4e24-47e8-a3ce-edcf6d1b11c7?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230223%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230223T124655Z&X-Amz-Expires=300&X-Amz-Signature=4a3cbb6aacaa7cd2a8b4c53459c1809769e5c263af499082f33ed65b087cf013&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dfhvhv_tripdata_2021-01.csv.gz&response-content-type=application%2Foctet-stream [following]\n",
      "--2023-02-23 23:46:55--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/035746e8-4e24-47e8-a3ce-edcf6d1b11c7?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230223%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230223T124655Z&X-Amz-Expires=300&X-Amz-Signature=4a3cbb6aacaa7cd2a8b4c53459c1809769e5c263af499082f33ed65b087cf013&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dfhvhv_tripdata_2021-01.csv.gz&response-content-type=application%2Foctet-stream\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 129967421 (124M) [application/octet-stream]\n",
      "Saving to: 'fhvhv_tripdata_2021-01.csv.gz.1’\n",
      "\n",
      "fhvhv_tripdata_2021 100%[===================>] 123.95M  2.11MB/s    in 27s     \n",
      "\n",
      "2023-02-23 23:47:23 (4.66 MB/s) - 'fhvhv_tripdata_2021-01.csv.gz.1’ saved [129967421/129967421]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhvhv/fhvhv_tripdata_2021-01.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d05ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l fhvhv_tripdata_2021-01.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d63776f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"header\", \"true\").csv(\"fhvhv_tripdata_2021-01.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a962fd15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|hvfhs_license_num|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|           HV0003|              B02682|2021-01-01 00:33:44|2021-01-01 00:49:07|         230|         166|   null|\n",
      "|           HV0003|              B02682|2021-01-01 00:55:19|2021-01-01 01:18:21|         152|         167|   null|\n",
      "|           HV0003|              B02764|2021-01-01 00:23:56|2021-01-01 00:38:05|         233|         142|   null|\n",
      "|           HV0003|              B02764|2021-01-01 00:42:51|2021-01-01 00:45:50|         142|         143|   null|\n",
      "|           HV0003|              B02764|2021-01-01 00:48:14|2021-01-01 01:08:42|         143|          78|   null|\n",
      "|           HV0005|              B02510|2021-01-01 00:06:59|2021-01-01 00:43:01|          88|          42|   null|\n",
      "|           HV0005|              B02510|2021-01-01 00:50:00|2021-01-01 01:04:57|          42|         151|   null|\n",
      "|           HV0003|              B02764|2021-01-01 00:14:30|2021-01-01 00:50:27|          71|         226|   null|\n",
      "|           HV0003|              B02875|2021-01-01 00:22:54|2021-01-01 00:30:20|         112|         255|   null|\n",
      "|           HV0003|              B02875|2021-01-01 00:40:12|2021-01-01 00:53:31|         255|         232|   null|\n",
      "|           HV0003|              B02875|2021-01-01 00:56:45|2021-01-01 01:17:42|         232|         198|   null|\n",
      "|           HV0003|              B02835|2021-01-01 00:29:04|2021-01-01 00:36:27|         113|          48|   null|\n",
      "|           HV0003|              B02835|2021-01-01 00:48:56|2021-01-01 00:59:12|         239|          75|   null|\n",
      "|           HV0004|              B02800|2021-01-01 00:15:24|2021-01-01 00:38:31|         181|         237|   null|\n",
      "|           HV0004|              B02800|2021-01-01 00:45:00|2021-01-01 01:06:45|         236|          68|   null|\n",
      "|           HV0003|              B02682|2021-01-01 00:11:53|2021-01-01 00:18:06|         256|         148|   null|\n",
      "|           HV0003|              B02682|2021-01-01 00:28:31|2021-01-01 00:41:40|          79|          80|   null|\n",
      "|           HV0003|              B02682|2021-01-01 00:50:49|2021-01-01 00:55:59|          17|         217|   null|\n",
      "|           HV0005|              B02510|2021-01-01 00:08:40|2021-01-01 00:39:39|          62|          29|   null|\n",
      "|           HV0003|              B02836|2021-01-01 00:53:48|2021-01-01 01:11:40|          22|          22|   null|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ecefe55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(hvfhs_license_num='HV0003', dispatching_base_num='B02682', pickup_datetime='2021-01-01 00:33:44', dropoff_datetime='2021-01-01 00:49:07', PULocationID='230', DOLocationID='166', SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02682', pickup_datetime='2021-01-01 00:55:19', dropoff_datetime='2021-01-01 01:18:21', PULocationID='152', DOLocationID='167', SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02764', pickup_datetime='2021-01-01 00:23:56', dropoff_datetime='2021-01-01 00:38:05', PULocationID='233', DOLocationID='142', SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02764', pickup_datetime='2021-01-01 00:42:51', dropoff_datetime='2021-01-01 00:45:50', PULocationID='142', DOLocationID='143', SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02764', pickup_datetime='2021-01-01 00:48:14', dropoff_datetime='2021-01-01 01:08:42', PULocationID='143', DOLocationID='78', SR_Flag=None)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b9c3c00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('hvfhs_license_num', StringType(), True), StructField('dispatching_base_num', StringType(), True), StructField('pickup_datetime', StringType(), True), StructField('dropoff_datetime', StringType(), True), StructField('PULocationID', StringType(), True), StructField('DOLocationID', StringType(), True), StructField('SR_Flag', StringType(), True)])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "165b0c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# We don't have access to the uncompressed csv, so we get the compressed version, then re-write the same file\n",
    "# as csv without the compression\n",
    "df_pandas = pd.read_csv(\"fhvhv_tripdata_2021-01.csv.gz\", compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ff6dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas.to_csv(\"fhvhv_tripdata_2021-01.csv\", compression=None, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27bb588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just get the first 100 data including the column name row at index 0\n",
    "!head -n 101 fhvhv_tripdata_2021-01.csv > head.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fb33efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101 head.csv\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l head.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c451287f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-read as csv\n",
    "df_pandas = pd.read_csv(\"head.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e29dcbb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hvfhs_license_num        object\n",
       "dispatching_base_num     object\n",
       "pickup_datetime          object\n",
       "dropoff_datetime         object\n",
       "PULocationID              int64\n",
       "DOLocationID              int64\n",
       "SR_Flag                 float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pandas.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d66cb273",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vuvuzella/Documents/projects/de_zoomcamp/week_5/application/spark_app/.venv/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/home/vuvuzella/Documents/projects/de_zoomcamp/week_5/application/spark_app/.venv/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    }
   ],
   "source": [
    "# Create a spark dataframe using pandas dataframe\n",
    "df_spark = spark.createDataFrame(df_pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "944ccd0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('hvfhs_license_num', StringType(), True), StructField('dispatching_base_num', StringType(), True), StructField('pickup_datetime', StringType(), True), StructField('dropoff_datetime', StringType(), True), StructField('PULocationID', LongType(), True), StructField('DOLocationID', LongType(), True), StructField('SR_Flag', DoubleType(), True)])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2b70c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert schema to python data types\n",
    "# StructField('hvfhs_license_num', StringType(), True)\n",
    "# StructField('dispatching_base_num', StringType(), True)\n",
    "# StructField('pickup_datetime', TimeStampType(), True),\n",
    "# StructField('dropoff_datetime', TimeStampType(), True)\n",
    "# StructField('PULocationID', IntegerType(), True)\n",
    "# StructField('DOLocationID', IntegerType(), True)\n",
    "# StructField('SR_Flag', Stringype(), True)]) since some fields are empty\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType\n",
    "\n",
    "# Convert\n",
    "spark_schema = StructType([\n",
    "    StructField('hvfhs_license_num', StringType(), True),\n",
    "    StructField('dispatching_base_num', StringType(), True),\n",
    "    StructField('pickup_datetime', TimestampType(), True),\n",
    "    StructField('dropoff_datetime', TimestampType(), True),\n",
    "    StructField('PULocationID', IntegerType(), True),\n",
    "    StructField('DOLocationID', IntegerType(), True),\n",
    "    StructField('SR_Flag', StringType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee3c8205",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_schema_df_spark = spark.read.option(\"header\", \"true\").schema(spark_schema).csv(\"fhvhv_tripdata_2021-01.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8553b0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|hvfhs_license_num|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|           HV0003|              B02682|2021-01-01 00:33:44|2021-01-01 00:49:07|         230|         166|   null|\n",
      "|           HV0003|              B02682|2021-01-01 00:55:19|2021-01-01 01:18:21|         152|         167|   null|\n",
      "|           HV0003|              B02764|2021-01-01 00:23:56|2021-01-01 00:38:05|         233|         142|   null|\n",
      "|           HV0003|              B02764|2021-01-01 00:42:51|2021-01-01 00:45:50|         142|         143|   null|\n",
      "|           HV0003|              B02764|2021-01-01 00:48:14|2021-01-01 01:08:42|         143|          78|   null|\n",
      "|           HV0005|              B02510|2021-01-01 00:06:59|2021-01-01 00:43:01|          88|          42|   null|\n",
      "|           HV0005|              B02510|2021-01-01 00:50:00|2021-01-01 01:04:57|          42|         151|   null|\n",
      "|           HV0003|              B02764|2021-01-01 00:14:30|2021-01-01 00:50:27|          71|         226|   null|\n",
      "|           HV0003|              B02875|2021-01-01 00:22:54|2021-01-01 00:30:20|         112|         255|   null|\n",
      "|           HV0003|              B02875|2021-01-01 00:40:12|2021-01-01 00:53:31|         255|         232|   null|\n",
      "|           HV0003|              B02875|2021-01-01 00:56:45|2021-01-01 01:17:42|         232|         198|   null|\n",
      "|           HV0003|              B02835|2021-01-01 00:29:04|2021-01-01 00:36:27|         113|          48|   null|\n",
      "|           HV0003|              B02835|2021-01-01 00:48:56|2021-01-01 00:59:12|         239|          75|   null|\n",
      "|           HV0004|              B02800|2021-01-01 00:15:24|2021-01-01 00:38:31|         181|         237|   null|\n",
      "|           HV0004|              B02800|2021-01-01 00:45:00|2021-01-01 01:06:45|         236|          68|   null|\n",
      "|           HV0003|              B02682|2021-01-01 00:11:53|2021-01-01 00:18:06|         256|         148|   null|\n",
      "|           HV0003|              B02682|2021-01-01 00:28:31|2021-01-01 00:41:40|          79|          80|   null|\n",
      "|           HV0003|              B02682|2021-01-01 00:50:49|2021-01-01 00:55:59|          17|         217|   null|\n",
      "|           HV0005|              B02510|2021-01-01 00:08:40|2021-01-01 00:39:39|          62|          29|   null|\n",
      "|           HV0003|              B02836|2021-01-01 00:53:48|2021-01-01 01:11:40|          22|          22|   null|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_schema_df_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e8f5b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Spark clusters have multiple executors, each executor performs a single computer (i.e. downloading a file).\n",
    "If a source data only has 1 partition for all the data (1 file to download), then this operation will only be\n",
    "done by a single executor. This is not efficient and does not utilize the cluster at all.\n",
    "\n",
    "Instead, the data must be divided into multiple small paritions\n",
    "sparks' dataframe.repartition does this for you.\n",
    "repartition is a lazy command, it does not execute it until you need to access a specific partition\n",
    "\"\"\"\n",
    "repartitioned_spark_df = new_schema_df_spark.repartition(numPartitions=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08f87c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:>                                                        (0 + 12) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/24 00:19:55 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "23/02/24 00:19:55 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "23/02/24 00:19:55 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "23/02/24 00:19:55 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "23/02/24 00:19:55 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 63.33% for 12 writers\n",
      "23/02/24 00:20:00 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "23/02/24 00:20:00 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "23/02/24 00:20:00 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "23/02/24 00:20:00 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 6:============================>                           (12 + 12) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/24 00:20:01 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "23/02/24 00:20:01 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "23/02/24 00:20:01 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "23/02/24 00:20:01 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "23/02/24 00:20:01 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 63.33% for 12 writers\n",
      "23/02/24 00:20:03 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "23/02/24 00:20:03 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "23/02/24 00:20:03 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "23/02/24 00:20:03 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "repartitioned_spark_df.write.parquet(\"fhvhv/2021/01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd8b46d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet('fhvhv/2021/01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c5b5491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hvfhs_license_num: string (nullable = true)\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a83cf9f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+------------+------------+--------------------+\n",
      "|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|dispatching_base_num|\n",
      "+-------------------+-------------------+------------+------------+--------------------+\n",
      "|2021-01-02 16:28:47|2021-01-02 16:59:53|         237|          48|              B02875|\n",
      "|2021-01-02 12:35:26|2021-01-02 12:46:10|         226|         179|              B02871|\n",
      "|2021-01-02 17:41:06|2021-01-02 17:58:45|         119|         241|              B02883|\n",
      "|2021-01-02 00:52:10|2021-01-02 01:04:12|          45|         229|              B02878|\n",
      "|2021-01-02 23:09:08|2021-01-02 23:22:48|         258|         102|              B02883|\n",
      "|2021-01-03 15:55:59|2021-01-03 16:07:44|          82|          95|              B02878|\n",
      "|2021-01-03 06:46:58|2021-01-03 07:04:58|          16|         218|              B02876|\n",
      "|2021-01-02 12:26:08|2021-01-02 12:31:39|         240|         174|              B02869|\n",
      "|2021-01-01 02:15:40|2021-01-01 02:29:57|         241|         244|              B02764|\n",
      "|2021-01-02 22:47:22|2021-01-02 22:57:19|         221|         206|              B02835|\n",
      "|2021-01-02 22:02:58|2021-01-02 22:18:46|         238|         142|              B02864|\n",
      "|2021-01-01 13:37:12|2021-01-01 13:47:32|         223|         129|              B02889|\n",
      "|2021-01-03 01:19:25|2021-01-03 01:41:27|          97|          92|              B02884|\n",
      "|2021-01-01 23:34:12|2021-01-01 23:50:12|         138|           4|              B02764|\n",
      "|2021-01-02 13:27:20|2021-01-02 13:43:24|         254|          18|              B02887|\n",
      "|2021-01-02 07:56:07|2021-01-02 08:09:55|         167|         254|              B02512|\n",
      "|2021-01-02 20:24:26|2021-01-02 20:38:20|         119|         169|              B02875|\n",
      "|2021-01-02 11:28:48|2021-01-02 11:57:47|         232|          36|              B02884|\n",
      "|2021-01-01 10:22:31|2021-01-01 10:31:20|          76|          77|              B02888|\n",
      "|2021-01-03 18:24:17|2021-01-03 18:41:27|          49|         144|              B02875|\n",
      "+-------------------+-------------------+------------+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('pickup_datetime', 'dropoff_datetime', 'PULocationID', 'DOLocationID', 'dispatching_base_num') \\\n",
    "    .filter(df.hvfhs_license_num == 'HV0003') \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2089b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+------------+------------+\n",
      "|pickup_date|dropoff_date|PULocationID|DOLocationID|\n",
      "+-----------+------------+------------+------------+\n",
      "| 2021-01-02|  2021-01-02|          85|          77|\n",
      "| 2021-01-02|  2021-01-02|         237|          48|\n",
      "| 2021-01-02|  2021-01-02|         226|         179|\n",
      "| 2021-01-03|  2021-01-03|          17|         148|\n",
      "| 2021-01-02|  2021-01-02|         119|         241|\n",
      "| 2021-01-02|  2021-01-02|          45|         229|\n",
      "| 2021-01-02|  2021-01-02|         258|         102|\n",
      "| 2021-01-03|  2021-01-03|          82|          95|\n",
      "| 2021-01-03|  2021-01-03|          16|         218|\n",
      "| 2021-01-01|  2021-01-01|          42|         241|\n",
      "| 2021-01-01|  2021-01-01|         255|         256|\n",
      "| 2021-01-02|  2021-01-02|         230|           7|\n",
      "| 2021-01-02|  2021-01-02|         240|         174|\n",
      "| 2021-01-01|  2021-01-01|         241|         244|\n",
      "| 2021-01-02|  2021-01-02|         221|         206|\n",
      "| 2021-01-01|  2021-01-02|         182|         242|\n",
      "| 2021-01-02|  2021-01-02|         238|         142|\n",
      "| 2021-01-01|  2021-01-01|         223|         129|\n",
      "| 2021-01-01|  2021-01-01|          68|          48|\n",
      "| 2021-01-03|  2021-01-03|          97|          92|\n",
      "+-----------+------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# built in spark functions found in pyspark.sql.functions\n",
    "from pyspark.sql import types\n",
    "from pyspark.sql import functions as F\n",
    "df \\\n",
    "    .withColumn('pickup_date', F.to_date(df.pickup_datetime)) \\\n",
    "    .withColumn('dropoff_date', F.to_date(df.dropoff_datetime)) \\\n",
    "    .select('pickup_date', 'dropoff_date', 'PULocationID', 'DOLocationID') \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2668c12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s/a7a'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# user defined functions\n",
    "def crazy_stuff(dispatch_base_num):\n",
    "    num = int(dispatch_base_num[1:])\n",
    "    if num % 7:\n",
    "        return f\"s/{num:03x}\"\n",
    "    else:\n",
    "        return f\"e/{num:03x}\"\n",
    "crazy_stuff_udf = F.udf(crazy_stuff, returnType=types.StringType())\n",
    "crazy_stuff('B02682')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "64968a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 7:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+------------+------------+------------+\n",
      "|base_id|pickup_date|dropoff_date|PULocationID|DOLocationID|\n",
      "+-------+-----------+------------+------------+------------+\n",
      "|  s/9ce| 2021-01-02|  2021-01-02|          85|          77|\n",
      "|  s/b3b| 2021-01-02|  2021-01-02|         237|          48|\n",
      "|  s/b37| 2021-01-02|  2021-01-02|         226|         179|\n",
      "|  e/af0| 2021-01-03|  2021-01-03|          17|         148|\n",
      "|  s/b43| 2021-01-02|  2021-01-02|         119|         241|\n",
      "|  s/b3e| 2021-01-02|  2021-01-02|          45|         229|\n",
      "|  s/b43| 2021-01-02|  2021-01-02|         258|         102|\n",
      "|  s/b3e| 2021-01-03|  2021-01-03|          82|          95|\n",
      "|  s/b3c| 2021-01-03|  2021-01-03|          16|         218|\n",
      "|  s/9ce| 2021-01-01|  2021-01-01|          42|         241|\n",
      "|  s/9ce| 2021-01-01|  2021-01-01|         255|         256|\n",
      "|  s/9ce| 2021-01-02|  2021-01-02|         230|           7|\n",
      "|  s/b35| 2021-01-02|  2021-01-02|         240|         174|\n",
      "|  s/acc| 2021-01-01|  2021-01-01|         241|         244|\n",
      "|  e/b13| 2021-01-02|  2021-01-02|         221|         206|\n",
      "|  s/9ce| 2021-01-01|  2021-01-02|         182|         242|\n",
      "|  s/b30| 2021-01-02|  2021-01-02|         238|         142|\n",
      "|  s/b49| 2021-01-01|  2021-01-01|         223|         129|\n",
      "|  s/9ce| 2021-01-01|  2021-01-01|          68|          48|\n",
      "|  e/b44| 2021-01-03|  2021-01-03|          97|          92|\n",
      "+-------+-----------+------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df \\\n",
    "    .withColumn('pickup_date', F.to_date(df.pickup_datetime)) \\\n",
    "    .withColumn('dropoff_date', F.to_date(df.dropoff_datetime)) \\\n",
    "    .withColumn('base_id', crazy_stuff_udf(df.dispatching_base_num)) \\\n",
    "    .select('base_id', 'pickup_date', 'dropoff_date', 'PULocationID', 'DOLocationID') \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fa66ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
